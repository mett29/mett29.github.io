<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>dropout on Matt Log</title>
    <link>/tags/dropout/</link>
    <description>Recent content in dropout on Matt Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2019 22:55:08 +0200</lastBuildDate><atom:link href="/tags/dropout/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Overfitting in NNs</title>
      <link>/posts/overfitting/</link>
      <pubDate>Wed, 30 Oct 2019 22:55:08 +0200</pubDate>
      
      <guid>/posts/overfitting/</guid>
      <description>In this post we will talk about the problem of overfitting, explaining what it is, what are its causes and how we can deal with it. More precisely, the following techniques will be explained: early stopping, weight decay and dropout.
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &amp;lsquo;Artificial Neural Networks and Deep Learning&amp;rsquo; course at Polytechnic of Milan, the book &amp;lsquo;Deep Learning&amp;rsquo; (Goodfellow-et-al-2016) and from some other online resources.</description>
    </item>
    
  </channel>
</rss>
