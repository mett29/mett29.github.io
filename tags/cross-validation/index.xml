<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>cross-validation on Matt Log</title>
    <link>https://mett29.github.io/tags/cross-validation/</link>
    <description>Recent content in cross-validation on Matt Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2019 22:55:08 +0200</lastBuildDate><atom:link href="https://mett29.github.io/tags/cross-validation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Overfitting in NNs</title>
      <link>https://mett29.github.io/posts/overfitting/</link>
      <pubDate>Wed, 30 Oct 2019 22:55:08 +0200</pubDate>
      
      <guid>https://mett29.github.io/posts/overfitting/</guid>
      <description>In this post we will talk about the problem of overfitting, explaining what it is, what are its causes and how we can deal with it. More precisely, the following techniques will be explained: early stopping, weight decay and dropout.
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &amp;lsquo;Artificial Neural Networks and Deep Learning&amp;rsquo; course at Polytechnic of Milan, the book &amp;lsquo;Deep Learning&amp;rsquo; (Goodfellow-et-al-2016) and from some other online resources.</description>
    </item>
    
    <item>
      <title>Bias-Variance Tradeoff and Model Selection</title>
      <link>https://mett29.github.io/posts/bias-variance-and-model-selection/</link>
      <pubDate>Thu, 17 Oct 2019 21:40:08 +0200</pubDate>
      
      <guid>https://mett29.github.io/posts/bias-variance-and-model-selection/</guid>
      <description>In this post we will talk about the Bias-Variance tradeoff, explaining where it comes from and how we can manage it, introducing techniques for model selection (feature selection, regularization, dimensionality reduction) and model ensemble (bagging and boosting).
Disclaimer: the following notes were written following the slides provided by the professor Restelli at Polytechnic of Milan and the book &amp;lsquo;Pattern Recognition and Machine Learning&#39;.
Bias-Variance trade-off and Model Selection No Free Lunch Theorems Define $Acc_G(L)$ as the generalization accuracy of the learner $L$, which is the accuracy of $L$ on non-training samples.</description>
    </item>
    
  </channel>
</rss>
