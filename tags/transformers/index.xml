<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>transformers on Matt Log</title>
    <link>https://mett29.github.io/tags/transformers/</link>
    <description>Recent content in transformers on Matt Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Sep 2023 21:46:00 +0200</lastBuildDate><atom:link href="https://mett29.github.io/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What is the KV cache?</title>
      <link>https://mett29.github.io/posts/kv-cache/</link>
      <pubDate>Mon, 18 Sep 2023 21:46:00 +0200</pubDate>
      
      <guid>https://mett29.github.io/posts/kv-cache/</guid>
      <description>Recently we&amp;rsquo;ve seen researchers and engineers scaling transformer-based models to hundreds of billions of parameters. The transformer architecture is exactly what made this possible, thanks to its sequence parallelism (here is an introduction to the transformer architecture). However, if it certainly enables an efficient training procedure, the same cannot be said about the inference process.
Background Recall the definition of Attention given in the &amp;ldquo;Attention Is All You Need&amp;rdquo; paper:</description>
    </item>
    
  </channel>
</rss>
