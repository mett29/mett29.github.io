<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Activation Functions | Matt Log</title>
<meta name="keywords" content="activation functions, sigmoid, tanh, ReLU, Leaky ReLU, ELU">
<meta name="description" content="In this post we will talk about activation functions, explaining what they are and what are the most commonly used (e.g. ReLU).
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.">
<meta name="author" content="">
<link rel="canonical" href="https://mett29.github.io/posts/activation-functions/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.0f21954db480b5bf5a45ab9ac4b9a7141baaef4db07466e008890636dc132e5d.css" integrity="sha256-DyGVTbSAtb9aRauaxLmnFBuq702wdGbgCIkGNtwTLl0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://mett29.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mett29.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mett29.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mett29.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://mett29.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="Activation Functions" />
<meta property="og:description" content="In this post we will talk about activation functions, explaining what they are and what are the most commonly used (e.g. ReLU).
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mett29.github.io/posts/activation-functions/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-11-05T22:55:08+02:00" />
<meta property="article:modified_time" content="2019-11-05T22:55:08+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Activation Functions"/>
<meta name="twitter:description" content="In this post we will talk about activation functions, explaining what they are and what are the most commonly used (e.g. ReLU).
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Activation Functions",
      "item": "https://mett29.github.io/posts/activation-functions/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Activation Functions",
  "name": "Activation Functions",
  "description": "In this post we will talk about activation functions, explaining what they are and what are the most commonly used (e.g. ReLU).\nDisclaimer: These notes are for the most part a collection of concepts taken from the slides of the \u0026lsquo;Artificial Neural Networks and Deep Learning\u0026rsquo; course at Polytechnic of Milan, the book \u0026lsquo;Deep Learning\u0026rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.",
  "keywords": [
    "activation functions", "sigmoid", "tanh", "ReLU", "Leaky ReLU", "ELU"
  ],
  "articleBody": "In this post we will talk about activation functions, explaining what they are and what are the most commonly used (e.g. ReLU).\nDisclaimer: These notes are for the most part a collection of concepts taken from the slides of the ‘Artificial Neural Networks and Deep Learning’ course at Polytechnic of Milan, the book ‘Deep Learning’ (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.\nActivation Functions There are many design choices that we can take when we are building a parametric machine learning model trained with gradient descent optimization. However, there is one specific design choice which is characteristic of Neural Networks: how to choose the type of hidden unit to use in the hidden layers of the model.\nNote:\n The design of hidden units is an extremely active area of research and does not yet have many definitive guiding theoretical principles. It is essentially impossible to predict which activation function will work best. The design process consists of trial and error.  Logistic Sigmoid and Hyperbolic Tangent Why do we like sigmoid function? Essentially because it’s easy to work with and has all the nice properties of activation functions: it’s non-linear, continuously differentiable, monotonic, and has a fixed output range.\nHowever, there is a serious drawback. Unlike piecewise linear units, sigmoidal units saturate across most of their domain—they saturate to a high value when $z$ is very positive, saturate to a low value when $z$ is very negative, and are only strongly sensitive to their input when $z$ is near 0. The widespread saturation of sigmoidal units can make gradient-based learning very difficult (vanishing gradient).\nThat’s why when a sigmoidal activation function must be used, the hyperbolic tangent activation function typically performs better than the logistic sigmoid. Tanh squashes a real-valued number to the range [-1, 1]. It’s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.\nIt resembles the identity function more closely, in the sense that $tanh(0) = 0$ while $\\sigma(0) = \\frac{1}{2}$. For this reason, training a deep neural network with this activation functions resembles training a linear model, thus making the training process easier.\nNote however that Tanh still has the vanishing gradient problem.\nRectified Linear Unit (ReLU) This is nowadays the most used activation function. Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid but with better performance.\nRectified linear units are easy to optimize because they are so similar to linear units. The only difference between a linear unit and a rectified linear unit is that a rectified linear unit outputs zero across half its domain. This makes the derivatives through a rectified linear unit remain large whenever the unit is active. The gradients are not only large but also consistent.\nPros:\n Faster SGD convergence (6x w.r.t. sigmoid/tanh) Sparse activation (only part of the hidden units are activated) Efficient gradient propagation (no vanishing or exploding gradient problems), and efficient computation Scale-invariant  Of course, there are also possible disadvantages:\n Non-differentiable at zero; however, it is differentiable anywhere else, and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. The range of ReLu is [0, inf). This means it can blow up the activation. For activations in the region $x \\le 0$ the gradient will be 0, hence the weights will not get adjusted during descent. The neurons which go in this state will stop responding to variations in error/input, and so they are said to be “died”.  Leaky ReLU and ELU Leaky ReLUs allow a small, positive gradient when the unit is not active. It is a fix for the “dying ReLU” problem.\nExponential Linear Unit is a function that tends to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number (tuned by hand).\n",
  "wordCount" : "688",
  "inLanguage": "en",
  "datePublished": "2019-11-05T22:55:08+02:00",
  "dateModified": "2019-11-05T22:55:08+02:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mett29.github.io/posts/activation-functions/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Matt Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mett29.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mett29.github.io/" accesskey="h" title="Matt Log (Alt + H)">Matt Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mett29.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://mett29.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Activation Functions
    </h1>
    <div class="post-meta"><span title='2019-11-05 22:55:08 +0200 +0200'>November 5, 2019</span>

</div>
  </header> 
  <div class="post-content"><p>In this post we will talk about <strong>activation functions</strong>, explaining what they are and what are the most commonly used (e.g. ReLU).</p>
<p><strong>Disclaimer:</strong> <em>These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.</em></p>
<h1 id="activation-functions">Activation Functions<a hidden class="anchor" aria-hidden="true" href="#activation-functions">#</a></h1>


<img src="/img/activation-functions/artificial-neuron-model.png" style="display: block; margin-left: auto; margin-right: auto;width: 600px;height: 300px;">

<p>There are many design choices that we can take when we are building a parametric machine learning model trained with gradient descent optimization. However, there is one specific design choice which is characteristic of Neural Networks: how to choose the type of hidden unit to use in the hidden layers of the model.</p>
<p><strong>Note:</strong></p>
<ul>
<li>The design of hidden units is an extremely active area of research and does not yet have many definitive guiding theoretical principles.</li>
<li>It is essentially impossible to predict which activation function will work best. The design process consists of trial and error.</li>
</ul>
<h2 id="logistic-sigmoid-and-hyperbolic-tangent">Logistic Sigmoid and Hyperbolic Tangent<a hidden class="anchor" aria-hidden="true" href="#logistic-sigmoid-and-hyperbolic-tangent">#</a></h2>


<img src="/img/activation-functions/sigmoid.svg" style="display: block; margin-left: auto; margin-right: auto;width: 500px;height: 300px;">

<p>Why do we like <strong>sigmoid function</strong>? Essentially because it’s easy to work with and has all the nice properties of activation functions: it’s non-linear, continuously differentiable, monotonic, and has a fixed output range.</p>
<p>However, there is a serious drawback. Unlike piecewise linear units, sigmoidal units saturate across most of their domain—they saturate to a high value when $z$ is very positive, saturate to a low value when $z$ is very negative, and are only strongly sensitive to their input when $z$ is near 0. The widespread saturation of sigmoidal units can make gradient-based learning very difficult (vanishing gradient).</p>
<p>That&rsquo;s why when a sigmoidal activation function must be used, the <strong>hyperbolic tangent</strong> activation function typically performs better than the logistic sigmoid. Tanh squashes a real-valued number to the range [-1, 1]. It’s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.</p>


<img src="/img/activation-functions/tanh.png" style="display: block; margin-left: auto; margin-right: auto;width: 410px;height: 250px;">

<p>It resembles the identity function more closely, in the sense that $tanh(0) = 0$ while $\sigma(0) = \frac{1}{2}$. For this reason, training a deep neural network with this activation functions resembles training a linear model, thus making the training process easier.</p>
<p>Note however that Tanh still has the vanishing gradient problem.</p>
<h2 id="rectified-linear-unit-relu">Rectified Linear Unit (ReLU)<a hidden class="anchor" aria-hidden="true" href="#rectified-linear-unit-relu">#</a></h2>
<p>This is nowadays the most used activation function. Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid but with better performance.</p>


<img src="/img/activation-functions/relu.png" style="display: block; margin-left: auto; margin-right: auto;width: 350px;height: 200px;">

<p>Rectified linear units are easy to optimize because they are so similar to linear units. The only difference between a linear unit and a rectified linear unit is that a rectified linear unit outputs zero across half its domain. This makes the derivatives through a rectified linear unit remain large whenever the unit is active. The gradients are not only large but also consistent.</p>
<p>Pros:</p>
<ul>
<li>Faster SGD convergence (6x w.r.t. sigmoid/tanh)</li>
<li>Sparse activation (only part of the hidden units are activated)</li>
<li>Efficient gradient propagation (no vanishing or exploding gradient problems), and efficient computation</li>
<li>Scale-invariant</li>
</ul>
<p>Of course, there are also possible disadvantages:</p>
<ul>
<li>Non-differentiable at zero; however, it is differentiable anywhere else, and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1.</li>
<li>The range of ReLu is [0, inf). This means it can blow up the activation.</li>
<li>For activations in the region $x \le 0$ the gradient will be 0, hence the weights will not get adjusted during descent. The neurons which go in this state will stop responding to variations in error/input, and so they are said to be &ldquo;died&rdquo;.</li>
</ul>
<h2 id="leaky-relu-and-elu">Leaky ReLU and ELU<a hidden class="anchor" aria-hidden="true" href="#leaky-relu-and-elu">#</a></h2>
<p><strong>Leaky ReLUs</strong> allow a small, positive gradient when the unit is not active. It is a fix for the &ldquo;dying ReLU&rdquo; problem.</p>


<img src="/img/activation-functions/leaky-relu-formula.png" style="display: block; margin-left: auto; margin-right: auto;width: 250px;height: 60px;">



<img src="/img/activation-functions/leaky-relu.png" style="display: block; margin-left: auto; margin-right: auto;width: 300px;height: 200px;">

<p><strong>Exponential Linear Unit</strong> is a function that tends to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number (tuned by hand).</p>


<img src="/img/activation-functions/elu-formula.png" style="display: block; margin-left: auto; margin-right: auto;width: 270px;height: 60px;">



<img src="/img/activation-functions/elu.png" style="display: block; margin-left: auto; margin-right: auto;width: 280px;height: 200px;">



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://mett29.github.io/tags/activation-functions/">activation functions</a></li>
      <li><a href="https://mett29.github.io/tags/elu/">ELU</a></li>
      <li><a href="https://mett29.github.io/tags/leaky-relu/">Leaky ReLU</a></li>
      <li><a href="https://mett29.github.io/tags/relu/">ReLU</a></li>
      <li><a href="https://mett29.github.io/tags/sigmoid/">sigmoid</a></li>
      <li><a href="https://mett29.github.io/tags/tanh/">tanh</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://mett29.github.io/">Matt Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
