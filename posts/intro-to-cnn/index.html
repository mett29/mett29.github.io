<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introduction to CNN | Matt Log</title>
<meta name="keywords" content="CNN, image classification, convolution, pooling, receptive field">
<meta name="description" content="In this post I will give you an introduction to Convolutional Neural Networks (CNN). We will see the reasons behind the success of this architecture and the latter will be analyzed layer by layer.
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources.">
<meta name="author" content="">
<link rel="canonical" href="https://mett29.github.io/posts/intro-to-cnn/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.0f21954db480b5bf5a45ab9ac4b9a7141baaef4db07466e008890636dc132e5d.css" integrity="sha256-DyGVTbSAtb9aRauaxLmnFBuq702wdGbgCIkGNtwTLl0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://mett29.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mett29.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mett29.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mett29.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://mett29.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="Introduction to CNN" />
<meta property="og:description" content="In this post I will give you an introduction to Convolutional Neural Networks (CNN). We will see the reasons behind the success of this architecture and the latter will be analyzed layer by layer.
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mett29.github.io/posts/intro-to-cnn/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-11-23T22:55:08+02:00" />
<meta property="article:modified_time" content="2019-11-23T22:55:08+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to CNN"/>
<meta name="twitter:description" content="In this post I will give you an introduction to Convolutional Neural Networks (CNN). We will see the reasons behind the success of this architecture and the latter will be analyzed layer by layer.
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Introduction to CNN",
      "item": "https://mett29.github.io/posts/intro-to-cnn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introduction to CNN",
  "name": "Introduction to CNN",
  "description": "In this post I will give you an introduction to Convolutional Neural Networks (CNN). We will see the reasons behind the success of this architecture and the latter will be analyzed layer by layer.\nDisclaimer: These notes are for the most part a collection of concepts taken from the slides of the \u0026lsquo;Artificial Neural Networks and Deep Learning\u0026rsquo; course at Polytechnic of Milan, the book \u0026lsquo;Deep Learning\u0026rsquo; (Goodfellow-et-al-2016) and from some other online resources.",
  "keywords": [
    "CNN", "image classification", "convolution", "pooling", "receptive field"
  ],
  "articleBody": "In this post I will give you an introduction to Convolutional Neural Networks (CNN). We will see the reasons behind the success of this architecture and the latter will be analyzed layer by layer.\nDisclaimer: These notes are for the most part a collection of concepts taken from the slides of the ‘Artificial Neural Networks and Deep Learning’ course at Polytechnic of Milan, the book ‘Deep Learning’ (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.\nIntroduction to CNN The convolution operation Convolution is a mathematical operation on two functions ($f$ and $g$) that produces a third function expressing how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it.\nThe convolution of $f$ and $g$ is written $f∗g$ and it can be computed as follows:\n$$\ns(t) = (f∗g)(t) = \\int_{-\\infty}^\\infty f(\\tau) g(t-\\tau) d\\tau $$\nIn convolutional network terminology, the first argument (in this case the function $f$) is called input and the second argument (in this case the function $g$) is called kernel. The output is usually referred to as the feature map.\nOf course, since we work with computers time will be discretized, thus we can define the discrete convolution:\n$$\ns(t) = (f∗g)(t) = \\sum_{a = -\\infty}^\\infty f(a) g(t-a) $$\nWe often use convolutions over more than one axis at a time. For example, if we use a two-dimensional image $I$ as input, we probably also want to use a two-dimensional kernel $K$:\n$$ S(i,j) = (I∗K)(i,j) = \\sum_m \\sum_n I(m,n)K(i-m,j-n) $$\nor, since convolution is commutative:\n$$ S(i,j) = (K∗I)(i,j) = \\sum_m \\sum_n I(i-m,j-n)K(m,n) $$\nNote that what is actually implemented in many neural network libraries is the cross-correlation function, which is the same as convolution but without flipping the kernel:\n$$ S(i,j) = (I∗K)(i,j) = \\sum_m \\sum_n I(i+m,j+n)K(m,n) $$\nImage from the Deep Learning boox\nImage Classification: the problem Given an input image $I$, assign a label $l$ from a fixed set of categories.\nChallenges in image classification:\n Images are very high-dimensional data. The CIFAR10 dataset, for example, has very small images (32x32) but still $d = 32 \\times 32 \\times 3 = 3072$ Label ambiguity: a label might not uniquely identify an image Transformations: there are many transformations that change the image dramatically, but not its label Inter-class variability: images in the same class might be dramatically different Perceptual similarity: perceptual similarity is not related to pixel similarity. This is the reason for which if we use a Nearest Neighborhood Classifier, assigning to each test image the label of the closest image in the training set, the results will be bad.  Linear Classifier where $W$ are the weights, $\\boldsymbol{b}$ is the bias (both parameters of the classifier $K$), and $\\boldsymbol{x_i}$ is our unrolled image.\n$$ K(\\boldsymbol{x}) = W\\boldsymbol{x} + \\boldsymbol{b} $$\nThe classifier assigns to an input image the class corresponding to the largest score:\n$$ \\hat{y}_j = \\underset{i=1…L}{\\operatorname{argmax}} [s_j]_i $$\nbeing $[s_j]_i$ the i-th component of the vector $K(\\boldsymbol{x_j}) = W\\boldsymbol{x_j} + \\boldsymbol{b}$.\nAs said before, $W$ and $\\boldsymbol{b}$ are parameters of the classifier. They are defined by training our classifier in order to minimize some loss function over a whole training set TR:\n$$ [W,b] = argmin_{W \\in R^{L \\times d}, b \\in R^L} \\sum_{(x_i,y_i) \\in TR} L(x,y_i) $$\nDenoting $W(i,:)$ as the d-dimensional vector containing the weights of the score function for the i-th class, it can be seen as a template used in matching.\nTemplates learned on the CIFAR10 dataset\nAs we can see the model has learned that the background of planes and boats is blue, that cars are typically red and so on. It is definitely too simple to achieve higher performance and better templates. We thus need a better approach.\nFeature Extraction Probably, feeding the images directly to the classifier is not a good idea. We need some intermediate steps with which we can extract meaningful information and also reduce the data-dimension.\nOf course, these features can be hand-crafted according to the problem we are facing. However, this is in general not advisable, since usually the patterns are very complex, there are many variables and there is a high risk of overfitting. As we know, neural networks are instead a good tool in this scenario, thanks to the fact that deep learning techniques are able to independently learn a hierarchical set of meaningful features.\nConvolutional Neural Networks CNN are typically made of blocks that include:\n convolutional layers nonlinearities (activation functions) pooling layers (usually max pooling)  First of all, why convolution? There are 3 main reasons:\n  sparse connectivity: In traditional neural networks every output unit interacts with every input unit (fully connected). This does not happen in CNN, where we typically have sparse interactions (or sparse weights). This is accomplished by making the kernel smaller than the input. This is particularly important with images, since as we said we have to deal with high-dimensional data. This approach, indeed, means that we need to store fewer parameters, which both reduces the memory requirements of the model and improves its statistical efficiency. Moreover, it also means that computing the output requires fewer operations.\n  parameter sharing: In a traditional neural network, each element of the weight matrix is used exactly once when computing the output of a layer. In a CNN, each member of the kernel is used at every position of the input (except perhaps some of the boundary pixels, we will see more on this later). This means that rather than learning a separate set of parameters for every location, we learn only one set. This further reduces the storage requirements of the model to $k$ parameters.\n  equivariant representations: thanks to parameter sharing, layers in CNN have a property called equivariance to translation. We say that a function is equivariant when if the input changes, the output changes in the same way. With images, convolution creates a 2D map of where certain features appear in the input. If we move the object in the input, its representation will move the same amount in the output. A very good explanation of this property can be found HERE.\n  CNN Architecture LeCun, Yann, et al. \"Gradient-based learning applied to document recognition\" (1998)\nThe above architecture is the LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998, that classifies digits (http://yann.lecun.com/exdb/lenet/).\nLet’s analyze it.\n An image passing through a CNN is transformed in a sequence of volumes As the depth increases, the height and width of the volume decreases Each layer takes as input and returns a volume  Convolutional layer As we have already seen, the convolutional layers consists of a linear combination of all the values in a region of the input. The parameters of this layer are called filters or kernels, and they represent the weights of the linear combination. During training, a CNN finds the most useful filters for its task, and it learns how to combine them into more complex patterns.\nIf we assume for example to have an input image with size $32 \\times 32 \\times 3$ (depth 3 in case of RGB image), we can use for example a filter $3 \\times 3 \\times 3$ in order to compute convolution (note that the depth of the filter must be the same as the depth of the image). This means that our filter will move over the image and at each step we obtain 3 values, one for each level of the input. These values will be summed together and then with the bias, in order to obtain a single value, that will be part of the resulting feature map.\nVisualizing what just described:\nCredits: Towards Data Science - A Comprehensive Guide to Convolutional Neural Networks\nTwo things to notice from the above image:\n There are some zeros in the border of the image. The reason is that in order for a layer to have the same height and width as the previous layer, it is common to add zeros around the input (zero padding). The filter shifts of 1 pixel at a time. This value can be set and it’s called stride, which indeed represents the distance between two consecutive receptive fields.  Moreover, note that in this representation the volume of the output has not increased. This is due to the fact that we used only one filter.\nPooling layer The goal of pooling layers is pretty simple: reduce the spatial size of the volume. This is done by subsampling the input image, usually using the MAX operation. In addition to reducing the computational load, the memory usage and the number of parameters, pooling also makes neural networks a bit more tolerant with respect to image shift, which can greatly improve the statistical efficiency of the network under the assumption that the function that the layer learns is invariant to small translations. Also in this layer we can define the size and the stride.\nAnd then? More or less we are done. The basic CNN architecture is simply a stack of convolutional layers (each one followed by an activation function) and pooling layers (usually max pooling). Going through this pipeline, the image generally gets smaller and smaller, but deeper and deeper (more and more feature maps). At the end of this stack there is a standard feedforward neural network, with some fully connected layers and generally a final softmax layer, providing as output class probabilities.\nWith some changes, but this is exactly the architecture of LeNet-5, that we saw before. In the last years, many variants have been developed, like AlexNet, GoogLeNet, ResNet etc., which I will not explain here (maybe in future dedicated posts).\nThe receptive field More or less, we have already talked about the receptive field, but let’s add some useful insights. It’s interesting to know that concept of receptive field comes from an experiment on cats and then monkeys carried out by David H. Hubel and Torsten Wiesel (who received the Nobel prize for their work).\n They showed that many neurons in the visual cortex have a small local receptive field, meaning they react only to visual stimuli located in a limited region of the visual field. Moreover, the authors showed that some neurons react only to images of horizontal lines, while others react only to lines with different orientations. They also noticed that some neurons have larger receptive fields, and they react to more complex patterns that are combinations of the lower-level patterns.\n Hands-On Machine Learning (1st edition) - Aurélien Géron\n  We can now well understand what is the idea behind CNN. As we said, unlike in fully connected layers, where the value of each output depends on the entire input, in CNNs an output only depends on a region of the input, the receptive field, precisely. The deeper you go, the wider the receptive field: convolution, max pooling and stride greater than 1 all increase the receptive field.\nConclusions Much more can be said about CNNs, but since this is thought to be an intro to the topic I will stop here, leaving further details to the future.\n",
  "wordCount" : "1883",
  "inLanguage": "en",
  "datePublished": "2019-11-23T22:55:08+02:00",
  "dateModified": "2019-11-23T22:55:08+02:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mett29.github.io/posts/intro-to-cnn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Matt Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mett29.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mett29.github.io/" accesskey="h" title="Matt Log (Alt + H)">Matt Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mett29.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://mett29.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Introduction to CNN
    </h1>
    <div class="post-meta"><span title='2019-11-23 22:55:08 +0200 +0200'>November 23, 2019</span>

</div>
  </header> 
  <div class="post-content"><p>In this post I will give you an introduction to <strong>Convolutional Neural Networks (CNN)</strong>. We will see the reasons behind the success of this architecture and the latter will be analyzed layer by layer.</p>
<p><strong>Disclaimer:</strong> <em>These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.</em></p>
<h1 id="introduction-to-cnn">Introduction to CNN<a hidden class="anchor" aria-hidden="true" href="#introduction-to-cnn">#</a></h1>
<h2 id="the-convolution-operation">The convolution operation<a hidden class="anchor" aria-hidden="true" href="#the-convolution-operation">#</a></h2>
<p><strong>Convolution</strong> is a mathematical operation on two functions ($f$ and $g$) that produces a third function expressing how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it.</p>
<p>The convolution of $f$ and $g$ is written $f∗g$ and it can be computed as follows:</p>
<p>$$<br>
s(t) = (f∗g)(t) = \int_{-\infty}^\infty f(\tau) g(t-\tau) d\tau
$$</p>
<p>In convolutional network terminology, the first argument (in this case the function $f$) is called <strong>input</strong> and the second argument (in this case the function $g$) is called <strong>kernel</strong>. The output is usually referred to as the <strong>feature map</strong>.</p>


<img src="/img/intro-to-cnn/conv-box-signal.gif" style="display: block; margin-left: auto; margin-right: auto;">

<p>Of course, since we work with computers time will be discretized, thus we can define the discrete convolution:</p>
<p>$$<br>
s(t) = (f∗g)(t) = \sum_{a = -\infty}^\infty f(a) g(t-a)
$$</p>
<p>We often use convolutions over more than one axis at a time. For example, if we use a two-dimensional image $I$ as input, we probably also want to use a two-dimensional kernel $K$:</p>
<p>$$
S(i,j) = (I∗K)(i,j) = \sum_m \sum_n I(m,n)K(i-m,j-n)
$$</p>
<p>or, since convolution is commutative:</p>
<p>$$
S(i,j) = (K∗I)(i,j) = \sum_m \sum_n I(i-m,j-n)K(m,n)
$$</p>
<p>Note that what is actually implemented in many neural network libraries is the <strong>cross-correlation</strong> function, which is the same as convolution but without flipping the kernel:</p>
<p>$$
S(i,j) = (I∗K)(i,j) = \sum_m \sum_n I(i+m,j+n)K(m,n)
$$</p>


<img src="/img/intro-to-cnn/2d-conv.png" style="display: block; margin-left: auto; margin-right: auto; width: 400px; height: 400px;">
<p style="text-align: center">Image from the <a href="https://www.deeplearningbook.org/">Deep Learning boox</a></p>

<h2 id="image-classification-the-problem">Image Classification: the problem<a hidden class="anchor" aria-hidden="true" href="#image-classification-the-problem">#</a></h2>
<p>Given an input image $I$, assign a label $l$ from a fixed set of categories.</p>
<p>Challenges in image classification:</p>
<ul>
<li>Images are very high-dimensional data. The CIFAR10 dataset, for example, has very small images (32x32) but still $d = 32 \times 32 \times 3 = 3072$</li>
<li>Label ambiguity: a label might not uniquely identify an image</li>
<li>Transformations: there are many transformations that change the image dramatically, but not its label</li>
<li>Inter-class variability: images in the same class might be dramatically different</li>
<li>Perceptual similarity: perceptual similarity is not related to pixel similarity. This is the reason for which if we use a Nearest Neighborhood Classifier, assigning to each test image the label of the closest image in the training set, the results will be bad.</li>
</ul>
<h2 id="linear-classifier">Linear Classifier<a hidden class="anchor" aria-hidden="true" href="#linear-classifier">#</a></h2>


<img src="/img/intro-to-cnn/linear-classifier.png" style="display: block; margin-left: auto; margin-right: auto;">

<p>where $W$ are the weights, $\boldsymbol{b}$ is the bias (both parameters of the classifier $K$), and $\boldsymbol{x_i}$ is our unrolled image.</p>
<p>$$
K(\boldsymbol{x}) = W\boldsymbol{x} + \boldsymbol{b}
$$</p>
<p>The classifier assigns to an input image the class corresponding to the largest score:</p>
<p>$$
\hat{y}_j = \underset{i=1&hellip;L}{\operatorname{argmax}} [s_j]_i
$$</p>
<p>being $[s_j]_i$ the i-th component of the vector $K(\boldsymbol{x_j}) = W\boldsymbol{x_j} + \boldsymbol{b}$.</p>
<p>As said before, $W$ and $\boldsymbol{b}$ are parameters of the classifier. They are defined by training our classifier in order to minimize some loss function over a whole training set TR:</p>
<p>$$
[W,b] = argmin_{W \in R^{L \times d}, b \in R^L} \sum_{(x_i,y_i) \in TR} L(x,y_i)
$$</p>
<p>Denoting $W(i,:)$ as the d-dimensional vector containing the weights of the score function for the i-th class, it can be seen as a <strong>template</strong> used in matching.</p>


<img src="/img/intro-to-cnn/templates.png" style="display: block; margin-left: auto; margin-right: auto;">
<p style="text-align: center">Templates learned on the CIFAR10 dataset</p>

<p>As we can see the model has learned that the background of planes and boats is blue, that cars are typically red and so on. It is definitely too simple to achieve higher performance and better templates. We thus need a better approach.</p>
<h2 id="feature-extraction">Feature Extraction<a hidden class="anchor" aria-hidden="true" href="#feature-extraction">#</a></h2>
<p>Probably, feeding the images directly to the classifier is not a good idea. We need some intermediate steps with which we can extract meaningful information and also reduce the data-dimension.</p>
<p>Of course, these features can be hand-crafted according to the problem we are facing. However, this is in general not advisable, since usually the patterns are very complex, there are many variables and there is a high risk of overfitting. As we know, neural networks are instead a good tool in this scenario, thanks to the fact that deep learning techniques are able to independently learn a hierarchical set of meaningful features.</p>
<h2 id="convolutional-neural-networks">Convolutional Neural Networks<a hidden class="anchor" aria-hidden="true" href="#convolutional-neural-networks">#</a></h2>
<p>CNN are typically made of blocks that include:</p>
<ul>
<li>convolutional layers</li>
<li>nonlinearities (activation functions)</li>
<li>pooling layers (usually max pooling)</li>
</ul>
<p>First of all, why convolution? There are 3 main reasons:</p>
<ul>
<li>
<p><strong>sparse connectivity</strong>: In traditional neural networks every output unit interacts with every input unit (fully connected). This does not happen in CNN, where we typically have sparse interactions (or sparse weights). This is accomplished by making the kernel smaller than the input. This is particularly important with images, since as we said we have to deal with high-dimensional data. This approach, indeed, means that we need to store fewer parameters, which both reduces the memory requirements of the model and improves its statistical efficiency. Moreover, it also means that computing the output requires fewer operations.</p>
</li>
<li>
<p><strong>parameter sharing</strong>: In a traditional neural network, each element of the weight matrix is used exactly once when computing the output of a layer. In a CNN, each member of the kernel is used at every position of the input (except perhaps some of the boundary pixels, we will see more on this later). This means that rather than learning a separate set of parameters for every location, we learn only one set. This further reduces the storage requirements of the model to $k$ parameters.</p>
</li>
<li>
<p><strong>equivariant representations</strong>: thanks to parameter sharing, layers in CNN have a property called equivariance to translation. We say that a function is equivariant when if the input changes, the output changes in the same way. With images, convolution creates a 2D map of where certain features appear in the input. If we move the object in the input, its representation will move the same amount in the output. A very good explanation of this property can be found <a href="https://www.quora.com/How-is-a-convolutional-neural-network-able-to-learn-invariant-features">HERE</a>.</p>
</li>
</ul>
<h2 id="cnn-architecture">CNN Architecture<a hidden class="anchor" aria-hidden="true" href="#cnn-architecture">#</a></h2>


<img src="/img/intro-to-cnn/LeNet.jpg" style="display: block; margin-left: auto; margin-right: auto;">
<p style="text-align: center">LeCun, Yann, et al. "Gradient-based learning applied to document recognition" (1998)</p>

<p>The above architecture is the LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998, that classifies digits (<a href="http://yann.lecun.com/exdb/lenet/)">http://yann.lecun.com/exdb/lenet/)</a>.</p>
<p>Let&rsquo;s analyze it.</p>
<ul>
<li>An image passing through a CNN is transformed in a sequence of volumes</li>
<li>As the depth increases, the height and width of the volume decreases</li>
<li>Each layer takes as input and returns a volume</li>
</ul>


<img src="/img/intro-to-cnn/cnn-volumes.png" style="display: block; margin-left: auto; margin-right: auto; width: 380px; height: 260px;">

<h3 id="convolutional-layer">Convolutional layer<a hidden class="anchor" aria-hidden="true" href="#convolutional-layer">#</a></h3>
<p>As we have already seen, the convolutional layers consists of a linear combination of all the values in a region of the input. The parameters of this layer are called <strong>filters</strong> or <strong>kernels</strong>, and they represent the weights of the linear combination. During training, a CNN finds the most useful filters for its task, and it learns how to combine them into more complex patterns.</p>
<p>If we assume for example to have an input image with size $32 \times 32 \times 3$ (depth 3 in case of RGB image), we can use for example a filter $3 \times 3 \times 3$ in order to compute convolution (note that the depth of the filter must be the same as the depth of the image). This means that our filter will move over the image and at each step we obtain 3 values, one for each level of the input. These values will be summed together and then with the bias, in order to obtain a single value, that will be part of the resulting <strong>feature map</strong>.</p>
<p>Visualizing what just described:</p>


<img src="/img/intro-to-cnn/convolution.gif" style="display: block; margin-left: auto; margin-right: auto;">
<p style="text-align: center">Credits: Towards Data Science - <a href="https://bit.ly/34AMuct">A Comprehensive Guide to Convolutional Neural Networks</a></p>

<p>Two things to notice from the above image:</p>
<ul>
<li>There are some zeros in the border of the image. The reason is that in order for a layer to have the same height and width as the previous layer, it is common to add zeros around the input (<strong>zero padding</strong>).</li>
<li>The filter shifts of 1 pixel at a time. This value can be set and it&rsquo;s called <strong>stride</strong>, which indeed represents the distance between two consecutive receptive fields.</li>
</ul>
<p>Moreover, note that in this representation the volume of the output has not increased. This is due to the fact that we used only one filter.</p>
<h3 id="pooling-layer">Pooling layer<a hidden class="anchor" aria-hidden="true" href="#pooling-layer">#</a></h3>
<p>The goal of pooling layers is pretty simple: reduce the spatial size of the volume. This is done by subsampling the input image, usually using the <strong>MAX</strong> operation. In addition to reducing the computational load, the memory usage and the number of parameters, pooling also makes neural networks a bit more tolerant with respect to image shift, which can greatly improve the statistical efficiency of the network under the assumption that the function that the layer learns is invariant to small translations. Also in this layer we can define the size and the stride.</p>


<img src="/img/intro-to-cnn/max-pooling.jpg" style="display: block; margin-left: auto; margin-right: auto; width: 480px; height: 220px;">

<h3 id="and-then">And then?<a hidden class="anchor" aria-hidden="true" href="#and-then">#</a></h3>
<p>More or less we are done. The basic CNN architecture is simply a stack of convolutional layers (each one followed by an activation function) and pooling layers (usually max pooling). Going through this pipeline, the image generally gets smaller and smaller, but deeper and deeper (more and more feature maps). At the end of this stack there is a standard feedforward neural network, with some fully connected layers and generally a final softmax layer, providing as output class probabilities.</p>
<p>With some changes, but this is exactly the architecture of LeNet-5, that we saw before. In the last years, many variants have been developed, like AlexNet, GoogLeNet, ResNet etc., which I will not explain here (maybe in future dedicated posts).</p>
<h2 id="the-receptive-field">The receptive field<a hidden class="anchor" aria-hidden="true" href="#the-receptive-field">#</a></h2>
<p>More or less, we have already talked about the receptive field, but let&rsquo;s add some useful insights. It&rsquo;s interesting to know that concept of <strong>receptive field</strong> comes from an experiment on cats and then monkeys carried out by David H. Hubel and Torsten Wiesel (who received the Nobel prize for their work).</p>
<blockquote>
<p>They showed that many neurons in the visual cortex have a small <em>local receptive field</em>, meaning they react only to visual stimuli located in a limited region of the visual field. Moreover, the authors showed that some neurons react only to images of horizontal lines, while others react only to lines with different orientations. They also noticed that some neurons have larger receptive fields, and they react to more complex patterns that are combinations of the lower-level patterns.</p>
<blockquote>
<p>Hands-On Machine Learning (1st edition) - Aurélien Géron</p>
</blockquote>
</blockquote>
<p>We can now well understand what is the idea behind CNN. As we said, unlike in fully connected layers, where the value of each output depends on the entire input, in CNNs an output only depends on a region of the input, the receptive field, precisely. The deeper you go, the wider the receptive field: convolution, max pooling and stride greater than 1 all increase the receptive field.</p>
<h2 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h2>
<p>Much more can be said about CNNs, but since this is thought to be an intro to the topic I will stop here, leaving further details to the future.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://mett29.github.io/tags/cnn/">CNN</a></li>
      <li><a href="https://mett29.github.io/tags/convolution/">convolution</a></li>
      <li><a href="https://mett29.github.io/tags/image-classification/">image classification</a></li>
      <li><a href="https://mett29.github.io/tags/pooling/">pooling</a></li>
      <li><a href="https://mett29.github.io/tags/receptive-field/">receptive field</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://mett29.github.io/">Matt Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
