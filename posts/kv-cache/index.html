<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is the KV cache? | Matt Log</title>
<meta name="keywords" content="KV cache, transformers, LLM, attention">
<meta name="description" content="Recently we&rsquo;ve seen researchers and engineers scaling transformer-based models to hundreds of billions of parameters. The transformer architecture is exactly what made this possible, thanks to its sequence parallelism (here is an introduction to the transformer architecture). However, if it certainly enables an efficient training procedure, the same cannot be said about the inference process.
Background Recall the definition of Attention given in the &ldquo;Attention Is All You Need&rdquo; paper:">
<meta name="author" content="">
<link rel="canonical" href="https://mett29.github.io/posts/kv-cache/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.0f21954db480b5bf5a45ab9ac4b9a7141baaef4db07466e008890636dc132e5d.css" integrity="sha256-DyGVTbSAtb9aRauaxLmnFBuq702wdGbgCIkGNtwTLl0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://mett29.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mett29.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mett29.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mett29.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://mett29.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="What is the KV cache?" />
<meta property="og:description" content="Recently we&rsquo;ve seen researchers and engineers scaling transformer-based models to hundreds of billions of parameters. The transformer architecture is exactly what made this possible, thanks to its sequence parallelism (here is an introduction to the transformer architecture). However, if it certainly enables an efficient training procedure, the same cannot be said about the inference process.
Background Recall the definition of Attention given in the &ldquo;Attention Is All You Need&rdquo; paper:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mett29.github.io/posts/kv-cache/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-18T21:46:00+02:00" />
<meta property="article:modified_time" content="2023-09-18T21:46:00+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="What is the KV cache?"/>
<meta name="twitter:description" content="Recently we&rsquo;ve seen researchers and engineers scaling transformer-based models to hundreds of billions of parameters. The transformer architecture is exactly what made this possible, thanks to its sequence parallelism (here is an introduction to the transformer architecture). However, if it certainly enables an efficient training procedure, the same cannot be said about the inference process.
Background Recall the definition of Attention given in the &ldquo;Attention Is All You Need&rdquo; paper:"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What is the KV cache?",
      "item": "https://mett29.github.io/posts/kv-cache/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is the KV cache?",
  "name": "What is the KV cache?",
  "description": "Recently we\u0026rsquo;ve seen researchers and engineers scaling transformer-based models to hundreds of billions of parameters. The transformer architecture is exactly what made this possible, thanks to its sequence parallelism (here is an introduction to the transformer architecture). However, if it certainly enables an efficient training procedure, the same cannot be said about the inference process.\nBackground Recall the definition of Attention given in the \u0026ldquo;Attention Is All You Need\u0026rdquo; paper:",
  "keywords": [
    "KV cache", "transformers", "LLM", "attention"
  ],
  "articleBody": "Recently we’ve seen researchers and engineers scaling transformer-based models to hundreds of billions of parameters. The transformer architecture is exactly what made this possible, thanks to its sequence parallelism (here is an introduction to the transformer architecture). However, if it certainly enables an efficient training procedure, the same cannot be said about the inference process.\nBackground Recall the definition of Attention given in the “Attention Is All You Need” paper:\n$$ Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\nwhere $Q$, $K$, and $V$ are three matrices that are trained during the training process. The embeddings of each token (a vector) is multiplied by these three matrices to obtain three vectors $q_n$, $k_n$, and $v_n$.\nWhen computing self-attention, we compute the dot product of the query vector $q_n$ with the key vector of every other token before it in the input sequence ${k_n, k_{n+1}, …, k_N}$.\nEach product $q_i^T \\cdot k_j$ is divided by the square root of the dimension of the key vectors $\\sqrt{d_k}$ in order to have more stable gradients. Eventually, everything is passed through a softmax to normalize the scores:\n$$ a_{ij} = \\frac{\\exp(q_i^T k_j / \\sqrt{d_k})}{\\sum_{t=1}^{i}\\exp(q_i^T k_t / \\sqrt{d_k})} $$\nThe final output is derived by computing the weighted average over the value vectors:\n$$ o_i = \\sum_{j=1}^{i} a_{ij} v_j $$\nThe autoregressive nature of transformers Transfomer-based models are autoregressive models, meaning essentially that they use the past to predict the future.\nGiven a prompt $(x_1, …, x_n)$\ngenerate vectors k_1, ..., k_n and v_1, ..., v_n compute the probability of the first new token Since the tokens $(x_1, …, x_n)$ are all known, computing $P(x_{n+1}|x_1,\\dots,x_n)$ can be made with matrix-matrix multiplication and thus benefit from GPU parallelism.\nInstead, when we get to compute the remaining tokens $P(x_{n+t+1}|x_1,\\dots,x_{n+t})$, the data dependency forces us to use a matrix-vector multiplication, which is less efficient and leads to an underutilization of the GPU.\n Reference: Efficient Memory Management for Large Language Model Serving with PagedAttention\n The KV cache In the process we described above, one can notice that the key and value vectors $k_1,\\dots,k_{n+t-1}$ and $v_1,\\dots,v_{n+t-1}$ seem to be re-computed every time a new token is taken into consideration. Of course, this would be a waste of resources.\nConsider the below illustration:\nThe $K$ and $V$ matrices contain information about all the sequence, while the query vector contains just the information about the last token. The dot product between $q$ and $K$ corresponds to doing attention between the last token (i.e. “blue” in our example) and all the previous ones.\nNote two things:\n during the sequence generation one token at a time, the two matrices $K$ and $V$ do not change very much once we computed the embedding for the new token, it’s not going to change, no matter how many more tokens we generate  That is why the key and value vectors of existing tokens are often cached for generating future tokens. This approach leads to what is called the KV cache. Note that the KV cache of one token depends on all its previous tokens, hence if we have the same token appearing in two different positions inside the sequence, the corresponding KV caches will be different as well.\nHow much memory does KV cache use? Let’s consider a 13B parameter OPT model\n$$ \\displaylines{\\text{memory_usage_per_token} = \\text{num_vectors} * \\text{hidden_state_size} * \\text{num_layers} * \\text{precision (bytes)} \\\\ = 2 * 5120 * 40 * 2 = 800\\; \\text{KB}} $$\nwhere $\\text{num_vectors}$ refers to the key and value vectors.\nIn OPT a sequence can be made of up to 2048 tokens, hence we would need $800 * 2048 \\approx 1.6\\; \\text{GB}$ per single request.\n Figure from paper Efficient Memory Management for Large Language Model Serving with PagedAttention  A large KV cache is thus a limitation when dealing with LLM inference. Moreover, as pointed out by Kwon et al. in Efficient Memory Management for Large Language Model Serving with PagedAttention, the current trend in the GPU market is characterized by a stable growth in the computation speed (FLOPS) and a much slower increase of the memory capacity. That is why they believe\n the memory will become an increasingly significant bottleneck.\n In their paper, Kwon et al. proposes a new attention algorithm that is inspired by the paging mechanism of operating systems to efficiently manage KV cache. Their results are quite promising, showing a 2-4x throughput improvements over the SOTA. Check their paper for more details!\nReferences  Vaswani et al. - Attention Is All You Need Kwon et al. - Efficient Memory Management for Large Language Model Serving with PagedAttention The KV Cache: Memory Usage in Transformers Jay Alammar - The Illustrated Transformer  ",
  "wordCount" : "766",
  "inLanguage": "en",
  "datePublished": "2023-09-18T21:46:00+02:00",
  "dateModified": "2023-09-18T21:46:00+02:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mett29.github.io/posts/kv-cache/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Matt Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mett29.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mett29.github.io/" accesskey="h" title="Matt Log (Alt + H)">Matt Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mett29.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://mett29.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      What is the KV cache?
    </h1>
    <div class="post-meta"><span title='2023-09-18 21:46:00 +0200 CEST'>September 18, 2023</span>

</div>
  </header> 
  <div class="post-content"><p>Recently we&rsquo;ve seen researchers and engineers scaling transformer-based models to hundreds of billions of parameters. The transformer architecture is exactly what made this possible, thanks to its sequence parallelism (<a href="/posts/seq2seq-and-attention">here is an introduction to the transformer architecture</a>). However, if it certainly enables an efficient training procedure, the same cannot be said about the inference process.</p>
<h3 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h3>
<p>Recall the definition of Attention given in the <a href="https://arxiv.org/pdf/1706.03762.pdf">&ldquo;Attention Is All You Need&rdquo;</a> paper:</p>
<p>$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$</p>
<p>where $Q$, $K$, and $V$ are three matrices that are trained during the training process. The embeddings of each token (a vector) is multiplied by these three matrices to obtain three vectors $q_n$, $k_n$, and $v_n$.</p>
<p>When computing self-attention, we compute the dot product of the query vector $q_n$ with the key vector of every other token before it in the input sequence ${k_n, k_{n+1}, &hellip;, k_N}$.</p>
<p>Each product $q_i^T \cdot k_j$ is divided by the square root of the dimension of the key vectors $\sqrt{d_k}$ in order to have more stable gradients. Eventually, everything is passed through a softmax to normalize the scores:</p>
<p>$$
a_{ij} = \frac{\exp(q_i^T k_j / \sqrt{d_k})}{\sum_{t=1}^{i}\exp(q_i^T k_t / \sqrt{d_k})}
$$</p>
<p>The final output is derived by computing the weighted average over the value vectors:</p>
<p>$$
o_i = \sum_{j=1}^{i} a_{ij} v_j
$$</p>
<h3 id="the-autoregressive-nature-of-transformers">The autoregressive nature of transformers<a hidden class="anchor" aria-hidden="true" href="#the-autoregressive-nature-of-transformers">#</a></h3>
<p>Transfomer-based models are <strong>autoregressive models</strong>, meaning essentially that they use the past to predict the future.</p>
<p>Given a prompt $(x_1, &hellip;, x_n)$</p>
<pre><code>generate vectors k_1, ..., k_n and v_1, ..., v_n
compute the probability of the first new token
</code></pre><p>Since the tokens $(x_1, &hellip;, x_n)$ are all known, computing $P(x_{n+1}|x_1,\dots,x_n)$ can be made with matrix-matrix multiplication and thus benefit from GPU parallelism.</p>
<p>Instead, when we get to compute the remaining tokens $P(x_{n+t+1}|x_1,\dots,x_{n+t})$, the data dependency forces us to use a matrix-vector multiplication, which is less efficient and leads to an underutilization of the GPU.</p>
<blockquote>
<p>Reference: <a href="https://arxiv.org/pdf/2309.06180.pdf">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></p>
</blockquote>
<h3 id="the-kv-cache">The KV cache<a hidden class="anchor" aria-hidden="true" href="#the-kv-cache">#</a></h3>
<p>In the process we described above, one can notice that the key and value vectors $k_1,\dots,k_{n+t-1}$ and $v_1,\dots,v_{n+t-1}$ seem to be re-computed every time a new token is taken into consideration. Of course, this would be a waste of resources.</p>
<p>Consider the below illustration:</p>


<img src="/img/kv-cache/QKV.png" style="display: block; margin-left: auto; margin-right: auto; width: 550px;">

<p>The $K$ and $V$ matrices contain information about all the sequence, while the query vector contains just the information about the last token. The dot product between $q$ and $K$ corresponds to doing attention between the last token (i.e. &ldquo;blue&rdquo; in our example) and all the previous ones.</p>
<p>Note two things:</p>
<ul>
<li>during the sequence generation one token at a time, the two matrices $K$ and $V$ do not change very much</li>
<li>once we computed the embedding for the new token, it&rsquo;s not going to change, no matter how many more tokens we generate</li>
</ul>
<p>That is why the key and value vectors of existing tokens are often cached for generating future tokens. This approach leads to what is called the <strong>KV cache</strong>. Note that the KV cache of one token depends on all its previous tokens, hence if we have the same token appearing in two different positions inside the sequence, the corresponding KV caches will be different as well.</p>


<img src="/img/kv-cache/KV_cached.png" style="display: block; margin-left: auto; margin-right: auto; width: 550px;">

<h3 id="how-much-memory-does-kv-cache-use">How much memory does KV cache use?<a hidden class="anchor" aria-hidden="true" href="#how-much-memory-does-kv-cache-use">#</a></h3>
<p>Let&rsquo;s consider a 13B parameter <a href="https://arxiv.org/pdf/2205.01068.pdf">OPT model</a></p>
<p>$$
\displaylines{\text{memory_usage_per_token} = \text{num_vectors} * \text{hidden_state_size} * \text{num_layers} * \text{precision (bytes)} \\ = 2 * 5120 * 40 * 2 = 800\; \text{KB}}
$$</p>
<p>where $\text{num_vectors}$ refers to the key and value vectors.</p>
<p>In OPT a sequence can be made of up to 2048 tokens, hence we would need $800 * 2048 \approx 1.6\; \text{GB}$ per single request.</p>


<figure>
<img src="/img/kv-cache/kv-cache-memory-usage.png" style="display: block; margin-left: auto; margin-right: auto; width: 200px;">
<figcaption style="text-align: center">Figure from paper <a href="https://arxiv.org/pdf/2309.06180.pdf">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></figcaption>
</figure>

<p>A large KV cache is thus a limitation when dealing with LLM inference. Moreover, as pointed out by Kwon et al. in <a href="https://arxiv.org/pdf/2309.06180.pdf">Efficient Memory Management for Large Language Model Serving with PagedAttention</a>, the current trend in the GPU market is characterized by a stable growth in the computation speed (FLOPS) and a much slower increase of the memory capacity. That is why they believe</p>
<blockquote>
<p>the memory will become an increasingly significant bottleneck.</p>
</blockquote>
<p>In their paper, Kwon et al. proposes a new attention algorithm that is inspired by the <strong>paging</strong> mechanism of operating systems to efficiently manage KV cache. Their results are quite promising, showing a 2-4x throughput improvements over the SOTA. Check their paper for more details!</p>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. - Attention Is All You Need</a></li>
<li><a href="https://arxiv.org/pdf/2309.06180.pdf">Kwon et al. - Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
<li><a href="https://www.youtube.com/watch?v=80bIUggRJf4">The KV Cache: Memory Usage in Transformers</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar - The Illustrated Transformer</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://mett29.github.io/tags/attention/">attention</a></li>
      <li><a href="https://mett29.github.io/tags/kv-cache/">KV cache</a></li>
      <li><a href="https://mett29.github.io/tags/llm/">LLM</a></li>
      <li><a href="https://mett29.github.io/tags/transformers/">transformers</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://mett29.github.io/">Matt Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
