<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Batch Normalization | Matt Log</title>
<meta name="keywords" content="batch normalization">
<meta name="description" content="In this post we will talk about batch normalization, explaining what it is and how it works!
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.">
<meta name="author" content="">
<link rel="canonical" href="https://mett29.github.io/posts/batch-normalization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.0f21954db480b5bf5a45ab9ac4b9a7141baaef4db07466e008890636dc132e5d.css" integrity="sha256-DyGVTbSAtb9aRauaxLmnFBuq702wdGbgCIkGNtwTLl0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://mett29.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mett29.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mett29.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mett29.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://mett29.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="Batch Normalization" />
<meta property="og:description" content="In this post we will talk about batch normalization, explaining what it is and how it works!
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mett29.github.io/posts/batch-normalization/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-11-17T22:55:08+02:00" />
<meta property="article:modified_time" content="2019-11-17T22:55:08+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Batch Normalization"/>
<meta name="twitter:description" content="In this post we will talk about batch normalization, explaining what it is and how it works!
Disclaimer: These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Batch Normalization",
      "item": "https://mett29.github.io/posts/batch-normalization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Batch Normalization",
  "name": "Batch Normalization",
  "description": "In this post we will talk about batch normalization, explaining what it is and how it works!\nDisclaimer: These notes are for the most part a collection of concepts taken from the slides of the \u0026lsquo;Artificial Neural Networks and Deep Learning\u0026rsquo; course at Polytechnic of Milan, the book \u0026lsquo;Deep Learning\u0026rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.",
  "keywords": [
    "batch normalization"
  ],
  "articleBody": "In this post we will talk about batch normalization, explaining what it is and how it works!\nDisclaimer: These notes are for the most part a collection of concepts taken from the slides of the ‘Artificial Neural Networks and Deep Learning’ course at Polytechnic of Milan, the book ‘Deep Learning’ (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.\nBatch Normalization Batch normalization (Ioffe and Szegedy, 2015) is a method of adaptive reparametrization motivated by the difficulty of training very deep models.\nYou can find the paper here $\\rightarrow$ Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift.\n Ioffe and Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n  Thus, in order to understand batch normalization, we first need to understand what is the covariate shift phenomenon.\nCovariate Shift “Covariates” is just another name for the input “features”, often written as $X$. Covariate shift means the distribution of the features is different in different parts of the training/test data, breaking the i.i.d assumption.\nMore in general, in the whole field of data science this problem is very well known and it is called dataset shift (or drifting). It occurs when the distribution of the training set and the test set is different, so no matter how well you trained your model, in the test set it will perform poorly. This problem is sometimes not mentioned, expecially in online competitions, because in that case the datasets are usually well organized and cleaned, having the same distribution in both the sets. This is not necessarily true in a real world scenario, where data might not have that level of quality. This is the case for example of finance, due to the always different market conditions.\nThis is a good article about this problem: Covariate Shift – Unearthing hidden problems in Real World Data Science\nComing back to our neural networks, internal covariate shift refers to covariate shift occurring within a neural network, for example going from layer 2 to layer 3. This happens because, as the network learns and the weights are updated, the distribution of outputs of a specific layer in the network changes. This forces the higher layers to adapt to that drift, which slows down learning.\nSolution Starting from the fact that it is known (LeCun et al., 1998b; Wiesler \u0026 Ney, 2011) that the network training converges faster if its inputs are whitened, the idea behind batch normalization is to apply the same procedure to the inputs of each layer.\nImage from the paper\nAs we can see, we have two new parameters: $\\gamma$ and $\\beta$. The reason is that normalizing the mean and standard deviation of a unit can reduce the expressive power of the neural network containing that unit. To maintain this expressive power, it is common to replace $\\hat{x_i}$ with $\\gamma \\hat{x_i} + \\beta$.\nThe variables $\\gamma$ and $\\beta$ are learned parameters that allow the new variable to have any mean and standard deviation. Even if this can seem counterintuitive, the reason is that this new parametrization can represent the same family of functions of the input of the old one, but it has a different learning dynamics. More precisely, in the old parametrization the mean was determined by a complicated interaction between the parameters in the previous layers, while in the new one it is determined by the solely $\\beta$, thus it is much easier to learn with gradient descent.\nConclusions and observations In practice batch normalization has shown to:\n improve gradient flow through the network allow higher learning rates reduce the strong dependence on initialization act as a form of regularization, reducing the need for dropout  Moreover, it makes possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.\nHowever, a recent paper questioned the reason for which batch normalization works, stating that\n the real reason is that it makes the optimization landscape significantly smoother, inducing a more predictive and stable behavior of the gradients, allowing for faster training.\n How Does Batch Normalization Help Optimization?\n  ",
  "wordCount" : "772",
  "inLanguage": "en",
  "datePublished": "2019-11-17T22:55:08+02:00",
  "dateModified": "2019-11-17T22:55:08+02:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mett29.github.io/posts/batch-normalization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Matt Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mett29.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mett29.github.io/" accesskey="h" title="Matt Log (Alt + H)">Matt Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mett29.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://mett29.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Batch Normalization
    </h1>
    <div class="post-meta"><span title='2019-11-17 22:55:08 +0200 +0200'>November 17, 2019</span>

</div>
  </header> 
  <div class="post-content"><p>In this post we will talk about <strong>batch normalization</strong>, explaining what it is and how it works!</p>
<p><strong>Disclaimer:</strong> <em>These notes are for the most part a collection of concepts taken from the slides of the &lsquo;Artificial Neural Networks and Deep Learning&rsquo; course at Polytechnic of Milan, the book &lsquo;Deep Learning&rsquo; (Goodfellow-et-al-2016) and from some other online resources. I am simply putting together all the information to study for the exam and I thought it would be a good idea to upload them here since they can be useful for someone who is interested in this topic.</em></p>
<h1 id="batch-normalization">Batch Normalization<a hidden class="anchor" aria-hidden="true" href="#batch-normalization">#</a></h1>
<p>Batch normalization (Ioffe and Szegedy, 2015) is a method of adaptive reparametrization motivated by the difficulty of training very deep models.</p>
<p>You can find the paper here $\rightarrow$ <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>
<blockquote>
<p>Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift.</p>
<blockquote>
<p>Ioffe and Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</p>
</blockquote>
</blockquote>
<p>Thus, in order to understand batch normalization, we first need to understand what is the <strong>covariate shift</strong> phenomenon.</p>
<h2 id="covariate-shift">Covariate Shift<a hidden class="anchor" aria-hidden="true" href="#covariate-shift">#</a></h2>
<p>“Covariates” is just another name for the input “features”, often written as $X$. Covariate shift means the distribution of the features is different in different parts of the training/test data, breaking the i.i.d assumption.</p>
<p>More in general, in the whole field of data science this problem is very well known and it is called <strong>dataset shift (or drifting)</strong>. It occurs when the distribution of the training set and the test set is different, so no matter how well you trained your model, in the test set it will perform poorly. This problem is sometimes not mentioned, expecially in online competitions, because in that case the datasets are usually well organized and cleaned, having the same distribution in both the sets. This is not necessarily true in a real world scenario, where data might not have that level of quality. This is the case for example of finance, due to the always different market conditions.</p>
<p>This is a good article about this problem: <a href="https://www.analyticsvidhya.com/blog/2017/07/covariate-shift-the-hidden-problem-of-real-world-data-science/">Covariate Shift – Unearthing hidden problems in Real World Data Science</a></p>
<p>Coming back to our neural networks, <strong>internal covariate shift</strong> refers to covariate shift occurring within a neural network, for example going from layer 2 to layer 3. This happens because, as the network learns and the weights are updated, the distribution of outputs of a specific layer in the network changes. This forces the higher layers to adapt to that drift, which slows down learning.</p>
<h2 id="solution">Solution<a hidden class="anchor" aria-hidden="true" href="#solution">#</a></h2>
<p>Starting from the fact that it is known (<em>LeCun et al., 1998b; Wiesler &amp; Ney, 2011</em>) that the network training converges faster if its inputs are whitened, the idea behind batch normalization is to apply the same procedure to the inputs of each layer.</p>


<img src="/img/batch-normalization/algorithm.png" style="display: block; margin-left: auto; margin-right: auto;">
<p style="text-align: center">Image from the <a href="https://arxiv.org/pdf/1502.03167.pdf">paper</a></p>

<p>As we can see, we have two new parameters: $\gamma$ and $\beta$. The reason is that normalizing the mean and standard deviation of a unit can reduce the expressive power of the neural network containing that unit. To maintain this expressive power, it is common to replace $\hat{x_i}$ with $\gamma \hat{x_i} + \beta$.</p>
<p>The variables $\gamma$ and $\beta$ are learned parameters that allow the new variable to have any mean and standard deviation. Even if this can seem counterintuitive, the reason is that this new parametrization can represent the same family of functions of the input of the old one, but it has a different learning dynamics. More precisely, in the old parametrization the mean was determined by a complicated interaction between the parameters in the previous layers, while in the new one it is determined by the solely $\beta$, thus it is much easier to learn with gradient descent.</p>
<h2 id="conclusions-and-observations">Conclusions and observations<a hidden class="anchor" aria-hidden="true" href="#conclusions-and-observations">#</a></h2>
<p>In practice batch normalization has shown to:</p>
<ul>
<li>improve gradient flow through the network</li>
<li>allow higher learning rates</li>
<li>reduce the strong dependence on initialization</li>
<li>act as a form of regularization, reducing the need for dropout</li>
</ul>
<p>Moreover, it makes possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.</p>
<p><strong>However</strong>, a recent paper questioned the reason for which batch normalization works, stating that</p>
<blockquote>
<p>the real reason is that it makes the optimization landscape significantly smoother, inducing a more predictive and stable behavior of the gradients, allowing for faster training.</p>
<blockquote>
<p><a href="https://arxiv.org/pdf/1805.11604.pdf">How Does Batch Normalization Help Optimization?</a></p>
</blockquote>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://mett29.github.io/tags/batch-normalization/">batch normalization</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://mett29.github.io/">Matt Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
